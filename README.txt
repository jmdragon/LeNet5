# LeNet-5

We implemented both:

1. **LeNet1** â€” the *original* LeNet-5 architecture from  
   *Gradient-Based Learning Applied to Document Recognition (LeCun et al., 1998)*  
   using scaled tanh activations, average subsampling, and RBF-style output units.

2. **LeNet2** â€” a *modified* and more robust CNN using  
   ReLU, max-pooling, cross-entropy loss, and geometric data augmentation  
   to handle **unseen / transformed MNIST** digits.

Both models are trained, evaluated, and saved for grading and testing.

---

## Group Members
- **Joshua Mondragon** 
- **Yohanan Pinto** 

---

## ğŸ“ Project Structure

HW4/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ train/ # 10,000 MNIST training PNGs (28Ã—28 â†’ padded to 32Ã—32)
â”‚ â”œâ”€â”€ test/ # 10,000 MNIST test PNGs
â”‚ â”œâ”€â”€ train_label.txt # One label per line (0â€“9)
â”‚ â”œâ”€â”€ test_label.txt
â”‚ â””â”€â”€ digits_updated/ # DIGIT dataset used to build RBF prototypes
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ LeNet1.pth # Trained original LeNet-5
â”‚ â””â”€â”€ LeNet2.pth # Trained modified LeNet-5
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ lenet1_errors.npz # Train/test error curves for LeNet1
â”‚ â”œâ”€â”€ lenet2_errors.npz # Train/test error curves for LeNet2
â”‚ â”œâ”€â”€ lenet1_confusion_matrix.txt
â”‚ â””â”€â”€ most_confusing_lenet1/ # Worst misclassified examples for LeNet1
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data.py
â”‚ â”œâ”€â”€ mnist.py
â”‚ â”œâ”€â”€ prototypes_from_digit.py
â”‚ â”œâ”€â”€ lenet1.py
â”‚ â”œâ”€â”€ lenet2.py
â”‚ â”œâ”€â”€ train_lenet1.py
â”‚ â”œâ”€â”€ train_lenet2.py
â”‚ â”œâ”€â”€ eval_lenet1.py
â”‚ â”œâ”€â”€ test1.py
â”‚ â””â”€â”€ test2.py
â”‚
â””â”€â”€ README.md


---

## ğŸ§  Model Summaries

### **LeNet1 (Original Architecture)**
Implements the exact LeNet-5 structure:
- Scaled tanh activations  
- Average subsampling  
- C1 â†’ S2 â†’ C3 â†’ S4 â†’ C5 â†’ F6  
- **RBF output layer**:  
  \( y_k = \| f_6 - \mu_k \|^2 \)  
- Trained with **MAP loss** (Eq. 9 in the paper)  
- Batch size = **1**, learning rate â‰ˆ **0.001**

**Prototype Generation:**  
RBF prototypes (10 Ã— 84) are generated from the **DIGIT** dataset by resizing digits to **7Ã—12**, averaging many samples, and binarizing to Â±1.

### **LeNet2 (Modified Architecture)**
A more modernized and robust variant:
- ReLU activations  
- Max pooling instead of average subsampling  
- Standard fully connected classifier  
- Cross-entropy loss  
- **RandomAffine** data augmentation:
  - Rotation Â±30Â°  
  - Translation Â±10%  
  - Scaling 0.8â€“1.2  
- Batch size = 64, SGD + Momentum

This model is more robust to unseen geometric distortions, as required by Problem 2.

---

## ğŸ”§ How to Run the Code

### **1. Train LeNet1 (original architecture)**  
```bash
python src/train_lenet1.py


### **2. 	Test LeNet1 (for grading)**
python src/test1.py

### **3.    Evaluate LeNet1 (confusion matrix + worst errors)**
python src/eval_lenet1.py

### **4.    Train LeNet2 (modified architecture)**
python src/train_lenet2.py

### **5.    Test LeNet2 (for grading)**
python src/test2.py

### **REMEMBER TO INSTALL**
pip install torch torchvision numpy pillow matplotlib
pip install huggingface_hub  # required for data.py (MNIST parquet)
